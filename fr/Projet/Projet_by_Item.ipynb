{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet\n",
    "\n",
    "## Objectifs :\n",
    "- Mise en place d'un système de recommandation (https://en.wikipedia.org/wiki/Recommender_system) bien commenté en Python\n",
    "- Rédaction du rapport de projet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projet (partie 1)\n",
    "\n",
    "L'objectif de ce projet est de recommander des images en fonction des préférences de l'utilisateur. Vous disposez de trois séances pratiques pour construire ce système. Vous devez vous assurer que toutes les tâches liées à l'acquisition, l'annotation, l'analyse et la visualisation des données sont automatisées.\n",
    "\n",
    "Les principales tâches du projet sont présentées ci-dessous :\n",
    "1. Collecte de données\n",
    "2. Étiquetage et annotation\n",
    "3. Analyses de données\n",
    "4. Visualisation des données\n",
    "5. Système de recommandation\n",
    "6. Tests\n",
    "7. Rapport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecte de données\n",
    "Vous devez collecter et télécharger un ensemble d'images. Vous avez les tâches suivantes à programmer, en automatisant le processus autant que possible :\n",
    "1. Créer un dossier appelé `images`\n",
    "2. Télécharger les images sous licence ouverte dans le dossier images (minimum 100 images).\n",
    "3. Enregistrez les métadonnées de chaque image comme la taille de l'image, le format de l'image ( jpeg, .png, etc.), l'orientation de l'image (paysage, portrait, carré, etc.), date de création, modèle d'appareil photo, etc. dans un ou plusieurs fichiers JSON. Vous pouvez utiliser les informations Exif présentes dans les fichiers d'images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Configuration initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import requests\n",
    "import shutil\n",
    "from urllib.parse import urlparse, unquote\n",
    "\n",
    "IMAGES_DIR = 'images'\n",
    "if not os.path.exists(IMAGES_DIR):\n",
    "    os.makedirs(IMAGES_DIR)\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fonction pour exécuter la requête SPARQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparql_results(endpoint_url, query):\n",
    "    \"\"\"\n",
    "    Exécute la requête SPARQL et renvoie les résultats.\n",
    "    \n",
    "    :param endpoint_url: URL de l'endpoint SPARQL\n",
    "    :param query: requête SPARQL\n",
    "    :return: résultats de la requête\n",
    "    \"\"\"\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (\n",
    "        sys.version_info[0],\n",
    "        sys.version_info[1],\n",
    "    )\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Script principal pour la collecte des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de la requête SPARQL (LIMIT à modfier selon le besoin)\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?grandeville ?grandevilleLabel ?pays ?paysLabel ?image {\n",
    "    ?grandeville wdt:P31 wd:Q1549591;\n",
    "    wdt:P17 ?pays;\n",
    "    wdt:P18 ?image.\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"fr\". }\n",
    "}\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "# Récupération des résultats de la requête SPARQL\n",
    "results = get_sparql_results(endpoint_url, query)\n",
    "\n",
    "# Traitement des résultats et création d'un DataFrame\n",
    "array = [(result[\"grandevilleLabel\"][\"value\"],\n",
    "          result[\"paysLabel\"][\"value\"],\n",
    "          result[\"image\"][\"value\"])\n",
    "        for result in results[\"results\"][\"bindings\"]]\n",
    "\n",
    "dataframe = pd.DataFrame(array, columns=[\"ville\", \"pays\", \"image\"])\n",
    "dataframe = dataframe.astype(dtype={\"ville\": \"<U200\", \"pays\": \"<U200\", \"image\": \"<U200\"})\n",
    "\n",
    "# Affichage du DataFrame pour vérification\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fonction de téléchargement des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    \"\"\"\n",
    "    Télécharge une image depuis une URL et la sauvegarde dans le dossier spécifié.\n",
    "    \n",
    "    :param url: URL de l'image à télécharger\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    request = requests.get(url, allow_redirects=True, headers=headers, stream=True)\n",
    "    \n",
    "    # Vérification du code de statut de la requête\n",
    "    # Si la requête a réussi (code 200), on sauvegarde l'image\n",
    "    if request.status_code == 200:\n",
    "        filename = os.path.join(IMAGES_DIR, unquote(urlparse(url).path.split('/')[-1]))\n",
    "        with open(filename, \"wb\") as image:\n",
    "            request.raw.decode_content = True\n",
    "            shutil.copyfileobj(request.raw, image)\n",
    "        \n",
    "        print(f\"Image sauvegardée : {filename}\")\n",
    "    else:\n",
    "        print(f\"Échec de la récupération de l'image : {request.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Téléchargement des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applique la fonction de téléchargement à chaque URL d'image\n",
    "for image_url in dataframe['image']:\n",
    "    download_image(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Extraction des métadata EXIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "def get_exif_data(image_path):\n",
    "    \"\"\"\n",
    "    Extrait les métadonnées EXIF d'une image.\n",
    "    \n",
    "    :param image_path: Chemin vers le fichier image.\n",
    "    :return: Dictionnaire des métadonnées EXIF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            exif_data = img._getexif()\n",
    "            # Les données EXIF peuvent être None si aucune n'est trouvée\n",
    "            if exif_data is not None:\n",
    "                # Convertit les valeurs EXIF en un format plus lisible\n",
    "                exif = {\n",
    "                    Image.ExifTags.TAGS[k]: v\n",
    "                    for k, v in exif_data.items()\n",
    "                    if k in Image.ExifTags.TAGS and isinstance(v, (str, int, float))\n",
    "                }\n",
    "                # Ajoute des informations supplémentaires\n",
    "                exif['File Size'] = os.path.getsize(image_path)\n",
    "                exif['Image Format'] = img.format\n",
    "                exif['Image Size'] = img.size\n",
    "                exif['Orientation'] = exif.get('Orientation', 'Undefined')\n",
    "                return exif\n",
    "            else:\n",
    "                return {}\n",
    "    except IOError:\n",
    "        print(f\"Impossible d'ouvrir l'image : {image_path}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Sauvegarde des métadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des métadata dans une liste\n",
    "metadata = []\n",
    "for image_filename in os.listdir(IMAGES_DIR):\n",
    "    image_path = os.path.join(IMAGES_DIR, image_filename)\n",
    "    exif_data = get_exif_data(image_path)\n",
    "    metadata.append({\n",
    "        'Filename': image_filename,\n",
    "        'Metadata': exif_data\n",
    "    })\n",
    "\n",
    "# Sauvegarde des métadonnées dans un fichier JSON\n",
    "with open('image_metadata.json', 'w') as json_file:\n",
    "    json.dump(metadata, json_file, indent=4)\n",
    "\n",
    "print(\"Métadonnées enregistrées dans image_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étiquetage et annotation\n",
    "Pour cette tâche, vous devez rechercher les sources disposant d'informations supplémentaires comme les balises, les catégories, etc.\n",
    "\n",
    "Dans cette tâche, vous devrez peut-être étiqueter, annoter et enregistrer des informations sur chaque image. Vous pouvez analyser les images en utilisant des algorithmes de regroupement pour trouver les couleurs prédominantes.\n",
    "\n",
    "Vous disposez déjà de certaines métadonnées provenant de l'EXIF des images de la précédente tâche. Dans cette tâche, votre objectif est d'obtenir des informations supplémentaires, comme les couleurs prédominantes, les tags. Et si vous demandiez aux utilisateurs de tagger les images ? Par exemple, les noms de couleurs, #cat, #fleur, #sous-fleur, rose etc.\n",
    "\n",
    "Comment prévoyez-vous de traiter ces tags ? Est-il possible d'automatiser ce processus ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Analyse des couleurs prédominantes\n",
    "\n",
    "Nous utilisons le KMeans clustering pour trouver les couleurs prédominantes des images. Il est également possible d'utiliser l'analyse en composantes principales (`PCA`) pour réduire le nombre de couleurs dans l'image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_dominant_colors(image_path, n_colors=5):\n",
    "    \"\"\"\n",
    "    Trouve les couleurs prédominantes dans une image.\n",
    "    \n",
    "    :param image_path: Chemin vers le fichier image.\n",
    "    :param n_colors: Nombre de couleurs prédominantes à trouver.\n",
    "    :return: Liste des couleurs prédominantes.\n",
    "    \"\"\"\n",
    "    with Image.open(image_path) as img:\n",
    "        # S'assurer que l'image est en mode RGB\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        np_pixels = np.array(img.resize((100, 100))).reshape(-1, 3)\n",
    "    \n",
    "    # Réduction de dimensionnalité pour accélérer le processus\n",
    "    # pca = PCA(n_components=3)\n",
    "    # pca.fit(np_pixels)\n",
    "    # pca_pixels = pca.transform(np_pixels)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_colors, n_init=10)\n",
    "    kmeans.fit(np_pixels)\n",
    "    # kmeans.fit(pca_pixels)\n",
    "    \n",
    "    # Récupération des centres de clusters comme couleurs dominates et assurer que les valeurs sont dans l'intervalle [0, 255]\n",
    "    dominant_colors = np.clip(kmeans.cluster_centers_, 0, 255).astype(int)\n",
    "    \n",
    "    # Conversion en hexadécimal\n",
    "    hex_colors = ['#{:02x}{:02x}{:02x}'.format(*color) for color in dominant_colors]\n",
    "    \n",
    "    return hex_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Récupération automatique des tags\n",
    "\n",
    "Nous utiliserons l'API Wikimedia Commons pour récupérer les tags associés à une image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_tags(filename):\n",
    "    \"\"\"\n",
    "    Récupère les tags associés à un fichier Wikimedia Commons.\n",
    "    \n",
    "    :param filename: Nom du fichier sur Wikimedia Commons.\n",
    "    :return: Liste des tags associés.\n",
    "    \"\"\"\n",
    "    start_url = \"https://commons.wikimedia.org/wiki/File:\"\n",
    "    url = f\"{start_url}{filename}\"\n",
    "    response = requests.get(url)\n",
    "    tags = []\n",
    "    \n",
    "    # Retourne les tags si la requête est réussie\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        category_links = soup.find_all('div', {'id': 'mw-normal-catlinks'})[0].find_all('a', href=True)\n",
    "        tags = [link.text for link in category_links if link['href'].startswith('/wiki/Category:')]\n",
    "        \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Récupération de la taille et de l'orientation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour obtenir la taille et l'orientation de l'image\n",
    "def get_image_size_and_orientation(image_path):\n",
    "    \"\"\"\n",
    "    Récupération de la taille et l'orientation\n",
    "    \n",
    "    :param image_path: Chemin vers le fichier image.\n",
    "    :return: Tuple de la taille et de l'orientation.\n",
    "    \"\"\"\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "        orientation = 'square'\n",
    "        if width > height:\n",
    "            orientation = 'landscape'\n",
    "        elif height > width:\n",
    "            orientation = 'portrait'\n",
    "    return f\"{width}x{height}\", orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Création d'un dataframe pour l'annotation\n",
    "\n",
    "Nous utiliserons Pandas pour créer un DataFrame qui stockera les informations d'annotation, y compris les tags fournis par les utilisateurs et les couleurs prédominantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Création d'un DataFrame pour les annotations\n",
    "df_annotations = pd.DataFrame(columns=['Filename', 'Dominant Colors', 'Auto Tags', 'User Tags', 'Image Size', 'Orientation'])\n",
    "\n",
    "# Ajout des informations dans le DataFrame\n",
    "for item in metadata:\n",
    "    filename = item['Filename']\n",
    "    image_path = os.path.join(IMAGES_DIR, filename)\n",
    "    \n",
    "    # Trouve les couleurs prédominantes\n",
    "    dominant_colors = find_dominant_colors(image_path)\n",
    "    \n",
    "    # Trouve les tags associés\n",
    "    auto_tags = get_tags(filename)\n",
    "    \n",
    "    # Trouve les informations de taille et d'orientation\n",
    "    image_size, orientation = get_image_size_and_orientation(image_path)\n",
    "    \n",
    "    # Préparation de la nouvelle ligne à ajouter\n",
    "    new_row = pd.DataFrame([{\n",
    "        'Filename': filename,\n",
    "        'Dominant Colors': dominant_colors,\n",
    "        'Auto Tags': auto_tags,\n",
    "        'User Tags': [], # Sera rempli par interaction utilisateur\n",
    "        'Image Size': image_size,\n",
    "        'Orientation': orientation\n",
    "    }])\n",
    "    \n",
    "    # Utilisation de pd.concat pour ajouter la nouvelle ligne\n",
    "    df_annotations = pd.concat([df_annotations, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fonction pour ajouter des tags manuellement\n",
    "\n",
    "Nous allons créer une interface simple (à l'aide d'un widget Jupyter ou en ligne de commande) pour permettre aux utilisateurs d'ajouter des tags aux images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tags_to_image(df, index):\n",
    "    \"\"\"\n",
    "    Permet aux utilisateurs d'ajouter des tags à une image.\n",
    "    \n",
    "    :param df: DataFrame d'annotations.\n",
    "    :param index: Index de l'image dans le DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"Image: {df.at[index, 'Filename']}\")\n",
    "    plt.imshow(Image.open(os.path.join(IMAGES_DIR, df.at[index, 'Filename'])))\n",
    "    plt.axis('off')  # Désactivation des axes\n",
    "    plt.show()\n",
    "    \n",
    "    tags = input(\"Entrez des tags pour cette image, séparés par des virgules : \").split(',')\n",
    "    df.at[index, 'User Tags'] = tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Boucle pour l'étiquetage manuel\n",
    "\n",
    "Nous allons exécuter une boucle qui demande à l'utilisateur d'entrer des tags pour chaque image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ceci doit être exécuté interactivement dans un environnement Jupyter\n",
    "for index, row in df_annotations.iterrows():\n",
    "    add_tags_to_image(df_annotations, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Sauvegarde des annotations\n",
    "\n",
    "Finalement, nous allons sauvegarder les annotations dans un fichier JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des annotations\n",
    "annotations_json = df_annotations.to_json(orient='records', lines=True)\n",
    "with open('annotations.json', 'w') as file:\n",
    "    file.write(annotations_json)\n",
    "\n",
    "print(\"Annotations enregistrées dans annotations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment prévoyez-vous de traiter ces tags ? Est-il possible d'automatiser ce processus ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour traiter les tags récupérés automatiquement et ceux entrés par l'utilisateur, nous envisageons d'adopter une approche structurée qui permet à la fois l'automatisation et la flexibilité pour des ajustements manuels.\n",
    "\n",
    "**1. Automatisation de la récupération des tags**\n",
    "\n",
    "Comme démontré précédemment, le processus de récupération des tags à partir de Wikimedia Commons est déjà automatisé grâce à la fonction `get_tags`. Cette fonction extrait les catégories (tags) directement associées aux images, servant de tags initiaux. L'automatisation de ce processus garantit que chaque image est associée à des métadonnées pertinentes dès le début, sans intervention manuelle.\n",
    "\n",
    "\n",
    "**2. Traitement et normalisation des tags**\n",
    "\n",
    "Après la récupération des tags, un traitement supplémentaire peut être nécessaire pour assurer la cohérence et la pertinence des tags :\n",
    "- **Normalisation** : Convertir tous les tags en minuscules pour éviter les doublons dus à des différences de casse.\n",
    "- **Filtrage** : Supprimer les tags non pertinents ou trop généraux basés sur une liste de mots-clés à exclure. Cette liste peut être ajustée selon les besoins spécifiques du projet.\n",
    "- **Regroupement** : Pour des tags similaires ou synonymes, envisager de les regrouper sous un tag unifié pour simplifier l'analyse et la recherche.\n",
    "Ces étapes peuvent être partiellement automatisées avec des règles prédéfinies ou des listes de mots-clés, mais elles peuvent également bénéficier d'une revue manuelle pour des cas spécifiques.\n",
    "\n",
    "Ces étapes peuvent être partiellement automatisées avec des règles prédéfinies ou des listes de mots-clés, mais elles peuvent également bénéficier d'une revue manuelle pour des cas spécifiques.\n",
    "\n",
    "**3. Intégration des tags utilisateur**\n",
    "\n",
    "Les tags entrés manuellement par les utilisateurs enrichissent les métadonnées des images avec des perspectives uniques et peuvent combler les lacunes des tags automatiques. Pour intégrer ces tags :\n",
    "- **Collecte** : Utiliser une interface simple (formulaire web ou widget notebook par exemple), comme montré précédemment, pour permettre aux utilisateurs d'ajouter des tags.\n",
    "- **Validation** : Bien que les tags des utilisateurs soient précieux, il est important de les valider pour éviter le spam ou les entrées non pertinentes. Cette validation peut être semi-automatisée avec des filtres de mots-clés ou des vérifications de pertinence.\n",
    "- **Fusion** : Les tags utilisateurs sont fusionnés avec les tags automatiques, en veillant à éliminer les doublons et à maintenir la cohérence de l'ensemble des tags.\n",
    "\n",
    "**4. Sauvegarde et utilisation des tags**\n",
    "\n",
    "Une fois les tags traités et validés, ils seront sauvegardés avec les autres métadonnées des images. Ces tags peuvent ensuite être utilisés pour :\n",
    "- **Recherche et filtrage** : Améliorer les fonctionnalités de recherche et de filtrage dans la base de données d'images.\n",
    "- **Analyse de données** : Servir de base pour des analyses plus approfondies, comme identifier les tendances dans les images ou comprendre les préférences des utilisateurs.\n",
    "- **Amélioration du système de recommandation** : Enrichir les données disponibles pour le système de recommandation, permettant des suggestions plus précises et personnalisées.\n",
    "\n",
    "En résumé, le traitement des tags peut être majoritairement automatisé, surtout pour la récupération et le traitement initial. Cependant, l'intégration des contributions des utilisateurs et certains aspects du filtrage et de la normalisation des tags peuvent bénéficier d'une approche \"semi-automatique\" qui combine les règles prédéfinies avec une validation manuelle pour assurer la qualité et la pertinence des métadonnées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyses de données\n",
    "Demandez à l'utilisateur de sélectionner quelques images et d'ajouter des balises. Pour chaque utilisateur, vous êtes maintenant prêt à construire un profil de préférences d'utilisateur, basé sur cette sélection. Vous pouvez recueillir les informations suivantes manuellement, mais l'objectif de cette tâche consiste à les obtenir en utilisant les images sélectionnées de manière automatisée :\n",
    "1. Couleurs préférées\n",
    "2. Orientation de l'image préférée\n",
    "3. Tailles d'images préférées (vignettes, grandes images, images de taille moyenne images, etc.)\n",
    "4. Balises favorites\n",
    "5. ...\n",
    "\n",
    "Maintenant, grâce à votre connaissance des différents types de classificateurs et les algorithmes de regroupement, quelles informations supplémentaires ajouterez-vous pour chaque image ?\n",
    "\n",
    "Votre prochain objectif est d'analyser les informations des utilisateurs et leur les images préférées.\n",
    "Comment avez-vous créé des utilisateurs aléatoires ? Combien d'utilisateurs avez-vous créer ? Quelles informations avez-vous stockées pour chaque utilisateur ? Quels types d'analyses avez-vous effectuées ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sélection des images par l'utilisateur\n",
    "\n",
    "Pour commencer, nous permettons à l'utilisateur de sélectionner un ensemble d'images. Ces images serviront de base pour construire le profil de préférences de l'utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "def user_select_images(df):\n",
    "    selected_indices = []\n",
    "    for index, row in df.iterrows():\n",
    "        # Affichage de l'image\n",
    "        image_path = os.path.join(IMAGES_DIR, row['Filename'])\n",
    "        img = Image.open(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Désactivation des axes\n",
    "        plt.show()\n",
    "        \n",
    "        # Demande à l'utilisateur s'il souhaite sélectionner l'image\n",
    "        user_input = input(f\"Voulez-vous sélectionner l'image {row['Filename']} ? (y/n) : \").lower()\n",
    "        if user_input == 'y':\n",
    "            selected_indices.append(index)\n",
    "            \n",
    "        # Option pour arrêter le processus si l'utilisateur ne veut plus sélectionner d'images\n",
    "        continue_choice = input(\"Voulez-vous continuer à voir plus d'images ? (y/n) : \").lower()\n",
    "        if continue_choice != 'y':\n",
    "            break\n",
    "            \n",
    "    return selected_indices\n",
    "\n",
    "# Utilisation de la fonction\n",
    "selected_images_indices = user_select_images(df_annotations)\n",
    "print(\"Indices des images sélectionnées :\", selected_images_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce script parcourt toutes les lignes du DataFrame contenant les informations sur les images, affiche chaque image à l'aide de matplotlib, et demande à l'utilisateur s'il souhaite sélectionner l'image affichée. Si l'utilisateur répond par 'y' (yes), l'indice de l'image est ajouté à la liste des indices sélectionnés. L'utilisateur a également la possibilité d'arrêter le processus à tout moment s'il ne souhaite plus sélectionner d'autres images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analyse des préférences de l'utilisateur\n",
    "\n",
    "Nous allons analyser les préférences de l'utilisateur basées sur les images sélectionnées. Cela inclut les couleurs préférées, l'orientation de l'image, les tailles d'images et les balises favorites.\n",
    "\n",
    "    1. Couleurs préférées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preferred_colors(df, indices):\n",
    "    \"\"\"\n",
    "    Récupère les couleurs préférées des images sélectionnées.\n",
    "    \n",
    "    :param df: DataFrame d'annotations.\n",
    "    :param indices: Indices des images sélectionnées.\n",
    "    :return: Liste des couleurs préférées.\n",
    "    \"\"\"\n",
    "    color_preferences = []\n",
    "    for index in indices:\n",
    "        color_preferences.extend(df.loc[index, 'Dominant Colors'])\n",
    "    return list(set(color_preferences))\n",
    "\n",
    "preferred_colors = get_preferred_colors(df_annotations, selected_images_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Orientation de l'image préférée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preferred_orientation(df, selected_indices):\n",
    "    \"\"\"\n",
    "    Récupère l'orientation préférée des images sélectionnées.\n",
    "    \n",
    "    :param df: DataFrame d'annotations.\n",
    "    :param selected_indices: Indices des images sélectionnées.\n",
    "    :return: Orientation préférée.\n",
    "    \"\"\"\n",
    "    orientation_counts = df.loc[selected_indices, 'Orientation'].value_counts()\n",
    "    preferred_orientation = orientation_counts.idxmax()\n",
    "    return preferred_orientation\n",
    "\n",
    "# Utilisation de la fonction avec les indices d'images sélectionnés par l'utilisateur\n",
    "preferred_orientation = get_preferred_orientation(df_annotations, selected_images_indices)\n",
    "print(\"Orientation préférée :\", preferred_orientation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction compte le nombre d'occurrences de chaque orientation parmi les images sélectionnées et détermine l'orientation la plus fréquente comme étant la préférée de l'utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Tailles d'images préférées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_image_size(width, height):\n",
    "    \"\"\"\n",
    "    Catégorise la taille de l'image en petite, moyenne ou grande.\n",
    "    \n",
    "    :param width: Largeur de l'image.\n",
    "    :param height: Hauteur de l'image.\n",
    "    :return: Catégorie de taille.\n",
    "    \"\"\"\n",
    "    pixels = width * height\n",
    "    if pixels < 640 * 480:\n",
    "        return 'small'\n",
    "    elif pixels <= 1280 * 1024:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "def get_preferred_size(df, selected_indices):\n",
    "    \"\"\"\n",
    "    Récupère la taille d'image préférée des images sélectionnées.\n",
    "    \n",
    "    :param df: DataFrame d'annotations.\n",
    "    :param selected_indices: Indices des images sélectionnées.\n",
    "    :return: Taille d'image préférée.\n",
    "    \"\"\"\n",
    "    sizes = [categorize_image_size(*map(int, df.loc[index, 'Image Size'].split('x'))) for index in selected_indices]\n",
    "    size_counts = pd.Series(sizes).value_counts()\n",
    "    preferred_size = size_counts.idxmax()\n",
    "    return preferred_size\n",
    "\n",
    "# Utilisation de la fonction avec les indices d'images sélectionnés par l'utilisateur\n",
    "preferred_size = get_preferred_size(df_annotations, selected_images_indices)\n",
    "print(\"Taille d'image préférée :\", preferred_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette approche décompose la chaîne Image Size en largeur et hauteur, les convertit en entiers, calcule le nombre total de pixels, et catégorise ensuite la taille de l'image. La taille la plus fréquente parmi les sélections de l'utilisateur est considérée comme sa préférée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Balises favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_favorite_tags(df, indices):\n",
    "    \"\"\"\n",
    "    Récupère les tags préférés des images sélectionnées.\n",
    "    \n",
    "    :param df: DataFrame d'annotations.\n",
    "    :param indices: Indices des images sélectionnées.\n",
    "    :return: Liste des tags préférés.\n",
    "    \"\"\"\n",
    "    tags = []\n",
    "    for index in indices:\n",
    "        tags.extend(df.loc[index, 'Auto Tags'] + df.loc[index, 'User Tags'])\n",
    "    return list(set(tags))\n",
    "\n",
    "favorite_tags = get_favorite_tags(df_annotations, selected_images_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Construction du profil de préférences\n",
    "\n",
    "Après avoir recueilli les informations sur les préférences de l'utilisateur, nous assemblons ces informations dans un profil de préférences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_preferences_profile = {\n",
    "    'Preferred Colors': preferred_colors,\n",
    "    'Preferred Orientation': preferred_orientation,\n",
    "    'Preferred Size': preferred_size,\n",
    "    'Favorite Tags': favorite_tags\n",
    "}\n",
    "\n",
    "print(user_preferences_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maintenant, grâce à votre connaissance des différents types de classificateurs et les algorithmes de regroupement, quelles informations supplémentaires ajouterez-vous pour chaque image ?**\n",
    "\n",
    "Avec un profil de préférences utilisateur déjà établi, basé sur les couleurs dominantes, l'orientation, la taille des images, et les tags, nous pouvons envisager d'ajouter plusieurs dimensions supplémentaires pour enrichir notre analyse et améliorer le système de recommandation. Voici quelques idées d'informations supplémentaires qui pourraient être pertinentes :\n",
    "\n",
    "**1. Complexité visuelle**\n",
    "\n",
    "La complexité visuelle d'une image peut être un indicateur intéressant de préférence. Certaines personnes peuvent préférer des images simples et épurées, tandis que d'autres peuvent être attirées par des images plus complexes et détaillées. Des mesures de texture ou de variété de couleurs peuvent être utilisées pour évaluer cette complexité.\n",
    "\n",
    "**2. Sentiments et Emotions**\n",
    "\n",
    "Les images évoquent des sentiments et des émotions. L'analyse sentimentale des images, bien que plus complexe, peut permettre de classer les images en catégories émotionnelles (joyeuse, triste, paisible, etc.). Cela pourrait se faire par des classificateurs entraînés sur des ensembles de données annotés avec des étiquettes émotionnelles.\n",
    "\n",
    "**3. Sujets et Objets**\n",
    "\n",
    "Identifier les sujets ou objets principaux présents dans les images (animaux, bâtiments, paysages naturels, etc.) peut aider à affiner les préférences. Ceci peut être réalisé grâce à la vision par ordinateur et l'apprentissage profond, par exemple avec des modèles pré-entraînés comme ceux disponibles dans TensorFlow ou PyTorch.\n",
    "\n",
    "**4. Popularité et Engagement**\n",
    "\n",
    "Pour les images provenant de sources en ligne, des métriques de popularité et d'engagement (nombre de vues, likes, partages) pourraient également être pertinentes. Ces informations peuvent aider à identifier les images qui résonnent le plus avec le public en général.\n",
    "\n",
    "**5. Contexte Temporel ou Saisonnier**\n",
    "\n",
    "Le moment de l'année ou le contexte temporel de l'image peut influencer les préférences. Par exemple, des images de paysages enneigés pourraient être plus attrayantes en hiver. Cette information peut être déduite des tags ou des métadonnées de l'image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour analyser les informations des utilisateurs et leurs images préférées, nous allons simuler un ensemble d'utilisateurs avec des préférences générées de manière aléatoire. Cela nous permettra de montrer comment les données utilisateur peuvent être analysées et utilisées pour des recommandations.\n",
    "\n",
    "1. Création d'utilisateurs aléatoires\n",
    "\n",
    "D'abord, créons un ensemble d'utilisateurs aléatoires. Pour chaque utilisateur, nous allons générer des préférences aléatoires pour les couleurs, l'orientation des images, la taille des images, et les tags.\n",
    "\n",
    "Les informations suivantes sont stockées pour chaque utilisateur :\n",
    "- **user_id** : Un identifiant unique pour l'utilisateur.\n",
    "- **Preferred Colors** : Un ensemble de couleurs préférées sélectionnées aléatoirement.\n",
    "- **Preferred Orientation** : L'orientation préférée de l'image (portrait, paysage, ou carré).\n",
    "- **Preferred Size** : La taille préférée de l'image (petite, moyenne, ou grande).\n",
    "- **Favorite Tags** : Un ensemble de tags qui représentent les intérêts de l'utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Nombre d'utilisateurs à générer\n",
    "num_users = 5 # A modifier selon le nombre d'utilisateurs souhaité\n",
    "\n",
    "# Création des utilisateurs avec des préférences aléatoires\n",
    "users_profiles = []\n",
    "\n",
    "for i in range(1, num_users + 1):\n",
    "    # Sélection aléatoire d'images\n",
    "    selected_indices = random.sample(range(len(df_annotations)), k=random.randint(1, len(df_annotations))) # Entre 1 et le nombre total d'images\n",
    "    \n",
    "    user_profile = {\n",
    "        'user_id': i,\n",
    "        'Preferred Colors': get_preferred_colors(df_annotations, selected_indices),\n",
    "        'Preferred Orientation': get_preferred_orientation(df_annotations, selected_indices),\n",
    "        'Preferred Size': get_preferred_size(df_annotations, selected_indices),\n",
    "        'Favorite Tags': get_favorite_tags(df_annotations, selected_indices)\n",
    "    }\n",
    "    users_profiles.append(user_profile)\n",
    "\n",
    "# Affichage des préférences des utilisateurs générés\n",
    "users_preferences_df = pd.DataFrame(users_profiles)\n",
    "users_preferences_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Encodage des préférences\n",
    "\n",
    "Les préférences des utilisateurs, en particulier les tags et les couleurs préférés, sont des données catégorielles qui peuvent être mieux traitées par les algorithmes d'apprentissage automatique après encodage. Pour les tags et les couleurs, nous pouvons utiliser l'encodage One-Hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Vérification que les colonnes existent\n",
    "if 'Favorite Tags' in users_preferences_df.columns and 'Preferred Colors' in users_preferences_df.columns:\n",
    "    # Encodage One-Hot pour les tags favoris\n",
    "    mlb_tags = MultiLabelBinarizer()\n",
    "    tags_encoded = mlb_tags.fit_transform(users_preferences_df['Favorite Tags'])\n",
    "    tags_encoded_df = pd.DataFrame(tags_encoded, columns=mlb_tags.classes_)\n",
    "    \n",
    "    # Encodage One-Hot pour les couleurs préférées\n",
    "    mlb_colors = MultiLabelBinarizer()\n",
    "    colors_encoded = mlb_colors.fit_transform(users_preferences_df['Preferred Colors'])\n",
    "    colors_encoded_df = pd.DataFrame(colors_encoded, columns=mlb_colors.classes_)\n",
    "    \n",
    "    # Concaténation avec le DataFrame original sans supprimer les colonnes originales pour éviter KeyError\n",
    "    users_preferences_df = pd.concat([users_preferences_df, tags_encoded_df, colors_encoded_df], axis=1)\n",
    "else:\n",
    "    print(\"Erreur : Colonnes 'Favorite Tags' ou 'Preferred Colors' manquantes.\")\n",
    "\n",
    "# Affichage des préférences avec les colonnes encodées\n",
    "# users_preferences_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Clustering des utilisateurs\n",
    "\n",
    "Nous pouvons utiliser un algorithme de clustering pour regrouper les utilisateurs en fonction de leurs préférences. Cela pourrait révéler des patterns dans les préférences des utilisateurs et permettre des recommandations ciblées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Vérification et Sélection des Colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes à exclure explicitement, incluant l'ID utilisateur et toute autre colonne non numérique\n",
    "columns_to_exclude = ['user_id', 'Preferred Orientation', 'Preferred Size', 'Favorite Tags', 'Preferred Colors']\n",
    "\n",
    "# Sélection des colonnes pour le clustering, en excluant les colonnes non numériques ou contenant des listes\n",
    "columns_for_clustering = [col for col in users_preferences_df.columns if col not in columns_to_exclude]\n",
    "\n",
    "print(\"Colonnes sélectionnées pour le clustering :\", columns_for_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Exécution de KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du KMeans avec les colonnes correctement sélectionnées\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10).fit(users_preferences_df[columns_for_clustering])\n",
    "\n",
    "# Ajout des labels de cluster au DataFrame\n",
    "users_preferences_df['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Analyse des clusters\n",
    "\n",
    "Une fois les clusters d'utilisateurs identifiés, nous pouvons analyser les caractéristiques de chaque cluster pour identifier les insights spécifiques, et comprendre les tendances et les similitudes entre les utilisateurs. Cela peut aider à personnaliser les recommandations en fonction des différents groupes d'utilisateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    # Sélection du cluster actuel\n",
    "    cluster = users_preferences_df[users_preferences_df['Cluster'] == i]\n",
    "    print(f\"Analyse du Cluster {i} :\")\n",
    "    print(f\"Nombre d'utilisateurs : {cluster.shape[0]}\")\n",
    "    print(\"Utilisateur ID :\", ', '.join(cluster['user_id'].astype(str).values))\n",
    "\n",
    "    # Distribution des préférences d'orientation\n",
    "    orientation_distribution = cluster['Preferred Orientation'].value_counts(normalize=True) * 100\n",
    "    print(\"\\nDistribution des préférences d'orientation (%) : \", orientation_distribution.to_string())\n",
    "\n",
    "    # Distribution des préférences de taille\n",
    "    size_distribution = cluster['Preferred Size'].value_counts(normalize=True) * 100\n",
    "    print(\"\\nDistribution des préférences de taille (%) : \", size_distribution.to_string())\n",
    "\n",
    "    # Tags communs et leur fréquence\n",
    "    all_tags_in_cluster = sum(cluster['Favorite Tags'].tolist(), [])\n",
    "    tags_freq = pd.Series(all_tags_in_cluster).value_counts().head(5)\n",
    "    print(\"\\nTags communs :\")\n",
    "    print(tags_freq.to_string())\n",
    "\n",
    "    # Supposons que les préférences de couleurs sont enregistrées de manière similaire aux tags\n",
    "    if 'Preferred Colors' in cluster.columns:\n",
    "        all_colors_in_cluster = sum(cluster['Preferred Colors'].tolist(), [])\n",
    "        if all_colors_in_cluster:  # Vérification que la liste n'est pas vide\n",
    "            colors_freq = pd.Series(all_colors_in_cluster).value_counts().head(5)\n",
    "            print(\"\\nCouleurs communes :\")\n",
    "            print(colors_freq.to_string())\n",
    "        else:\n",
    "            print(\"\\nCouleurs communes : Informations sur les couleurs non disponibles\")\n",
    "    else:\n",
    "        print(\"\\nCouleurs communes : Informations sur les couleurs non disponibles\")\n",
    "\n",
    "    print(\"\\n----------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Explications**\n",
    "\n",
    "- **Nombre d'utilisateurs et ID** : Nous affichons le nombre d'utilisateurs dans chaque cluster et leurs identifiants pour faciliter le suivi et l'identification des segments d'utilisateurs.\n",
    "- **Préférences d'orientation et de taille** : Nous résumons les préférences d'orientation et de taille les plus courantes dans chaque cluster. Plus précisémment, pour chaque cluster, nous calculons la distribution des préférences d'orientation et de taille parmi les utilisateurs. L'utilisation de `value_counts(normalize=True)` permet de calculer les proportions, que nous multiplions par 100 pour obtenir des pourcentages.\n",
    "- **Tags communs** : Nous identifions les tags les plus fréquents dans le cluster et affichons leur fréquence. Cela donne une idée des intérêts ou des caractéristiques les plus communs parmi les utilisateurs du cluster.\n",
    "- **Couleurs communes** : Si les préférences de couleurs sont disponibles, nous calculons également les couleurs les plus fréquentes dans chaque cluster. Ceci est fait de manière similaire aux tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Création des utilisateurs aléatoires**\n",
    "\n",
    "- **Comment avez-vous créé des utilisateurs aléatoires ?**\n",
    "\n",
    "Nous avons généré des utilisateurs aléatoires en simulant des sélections d'images pour chaque utilisateur. Pour chaque utilisateur simulé, un ensemble aléatoire d'images a été sélectionné, et à partir de ces images, des préférences telles que les couleurs préférées, l'orientation de l'image préférée, les tailles d'images préférées, et les tags favoris ont été dérivées.\n",
    "\n",
    "- **Combien d'utilisateurs avez-vous créé ?**\n",
    "\n",
    "Nous avons simulé un total de `5` utilisateurs, bien que ce nombre puisse être ajusté en fonction des besoins spécifiques de l'analyse ou des tests.\n",
    "\n",
    "#### **Informations stockées pour chaque utilisateur**\n",
    "\n",
    "- **Quelles informations avez-vous stockées pour chaque utilisateur ?**\n",
    "\n",
    "Pour chaque utilisateur, les informations suivantes ont été stockées :\n",
    "- Les `couleurs préférées`, déterminées à partir des images sélectionnées.\n",
    "- L'`orientation` de l'image préférée (paysage, portrait, carré).\n",
    "- La `taille` d'image préférée (petite, moyenne, grande), basée sur les dimensions des images sélectionnées.\n",
    "- Les `tags favoris`, extraits à la fois automatiquement des images sélectionnées et à travers des entrées manuelles simulées.\n",
    "- Un `identifiant de cluster` attribué après avoir effectué un `clustering KMeans` sur les utilisateurs basé sur leurs préférences encodées.\n",
    "\n",
    "\n",
    "#### **Types d'analyses effectuées**\n",
    "\n",
    "- **Quels types d'analyses avez-vous effectuées ?**\n",
    "\n",
    "Plusieurs types d'analyses ont été effectuées sur les données simulées des utilisateurs :\n",
    "- **Clustering des utilisateurs** : Utilisation de l'algorithme `KMeans` pour grouper les utilisateurs en clusters basés sur leurs préférences encodées en `One-Hot`.\n",
    "- **Analyse des préférences par cluster** : Pour chaque cluster, nous avons analysé les préférences communes, telles que l'orientation et la taille d'image préférées, ainsi que les tags et couleurs les plus fréquents. Cette analyse a permis de dégager des tendances et des caractéristiques communes au sein de chaque groupe d'utilisateurs.\n",
    "- **Distribution des préférences** : Pour enrichir l'analyse, nous avons calculé et affiché la distribution des préférences d'orientation, de taille, des tags et couleurs préférés au sein de chaque cluster, offrant une vue plus nuancée des tendances générales.\n",
    "\n",
    "Ces analyses fournissent une compréhension approfondie des segments d'utilisateurs, en identifiant des patterns et des préférences communes qui peuvent être utilisées pour améliorer le système de recommandation d'images, en personnalisant les suggestions en fonction des profils de préférences détectés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des données\n",
    "Dans cette tâche, votre objectif est de visualiser les différentes caractéristiques de toutes les images\n",
    "téléchargées.\n",
    "1. Le nombre d'images disponibles pour chaque année\n",
    "2. Le nombre d'images disponibles pour les différents types : taille de l'image, l'orientation des images, les modèles d'appareils photo, etc.\n",
    "3. Caractéristiques des couleurs\n",
    "\n",
    "Les utilisateurs peuvent également visualiser les informations ci-dessus relatives à leurs images préférées.\n",
    "Dans cette tâche, vous devez également ajouter une fonctionnalité permettant aux utilisateurs de\n",
    "visualiser les informations liées à leur propre profil d'utilisateur."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
